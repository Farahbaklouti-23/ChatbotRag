# ===============================
# rag.py - Syst√®me RAG avec Chroma depuis Hugging Face Hub
# ===============================

import os
import shutil
import zipfile
from huggingface_hub import hf_hub_download

# --- Imports LangChain / Ollama ---
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain

# ===============================
# Configuration des mod√®les et param√®tres
# ===============================
HF_REPO_ID = "Farahbaklouti-2002/political-rag-index"  # ‚ö†Ô∏è √† remplacer par ton repo Hugging Face
HF_FILENAME = "chroma.zip"            # Le fichier que tu as upload√©
CHROMA_DIR = "chroma_index"           # R√©pertoire local o√π sera extrait l‚Äôindex

EMBEDDING_MODEL = "all-MiniLM-L6-v2"
OLLAMA_MODEL = "mistral"
TEMPERATURE = 0.1
TOP_K = 10

# ===============================
# Prompt personnalis√©
# ===============================
CUSTOM_PROMPT_TEMPLATE = """
Tu es un expert sp√©cialis√© dans les personnalit√©s politiques mondiales.  

**Extrais les informations suivantes si disponibles dans le contexte:**
- Nom complet
- Date de naissance
- Genre
- Pays
- R√©gion/Subdivision
- Postes politiques occup√©s

**R√®gles:**
1. Utilise UNIQUEMENT les informations du contexte
2. Si la question NE CONCERNE PAS une personnalit√© politique, r√©ponds UNIQUEMENT:
   "D√©sol√©, je ne peux r√©pondre qu'aux questions concernant des personnalit√©s politiques."
3. Si la personnalit√© demand√©e n'est pas pr√©sente dans la base, r√©ponds:
   "Le nom demand√© n'est pas disponible dans la base."
4. Pour les champs manquants, indique "Non sp√©cifi√©"
5. Pr√©sente les postes politiques sous forme de liste √† puces
6. Structure la r√©ponse clairement avec les sections suivantes:

Structure de r√©ponse OBLIGATOIRE:
Nom: [nom complet]
Naissance: [date de naissance]
Genre: [genre]
Pays: [pays]
R√©gion: [r√©gion/subdivision]
Postes:
‚Ä¢ [poste 1]
‚Ä¢ [poste 2]
‚Ä¢ ...

Contexte:
{context}

Question: {question}

R√©ponse:
"""

# ===============================
# T√©l√©chargement & extraction de l‚Äôindex Chroma
# ===============================
def download_and_extract_chroma(repo_id=HF_REPO_ID, filename=HF_FILENAME, local_dir=CHROMA_DIR):
    """
    T√©l√©charge chroma.zip depuis Hugging Face Hub et l'extrait dans local_dir.
    """
    if not os.path.exists(local_dir) or not os.listdir(local_dir):
        print("üì• T√©l√©chargement de l'index Chroma depuis Hugging Face...")
        downloaded_file = hf_hub_download(repo_id=repo_id, filename=filename)

        # Nettoyer l'ancien dossier si besoin
        if os.path.exists(local_dir):
            shutil.rmtree(local_dir)

        os.makedirs(local_dir, exist_ok=True)

        # D√©compression
        with zipfile.ZipFile(downloaded_file, "r") as zip_ref:
            zip_ref.extractall(local_dir)

    return local_dir

# ===============================
# Initialisation du syst√®me RAG
# ===============================
def initialize_rag_system():
    """
    Initialise la cha√Æne RAG en utilisant l‚Äôindex Chroma h√©berg√© sur Hugging Face Hub.
    """
    # 1. Charger embeddings
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    # 2. T√©l√©charger / extraire Chroma index
    local_chroma_dir = download_and_extract_chroma()

    # 3. Charger Chroma
    chroma_index = Chroma(
        persist_directory=local_chroma_dir,
        embedding_function=embeddings
    )

    # 4. Charger LLM
    llm = ChatOllama(model=OLLAMA_MODEL, temperature=TEMPERATURE)

    # 5. Construire cha√Æne RAG
    return create_rag_chain(llm, chroma_index)

# ===============================
# Cha√Æne RAG conversationnelle
# ===============================
def create_rag_chain(llm, chroma_index):
    QA_PROMPT = PromptTemplate(
        template=CUSTOM_PROMPT_TEMPLATE,
        input_variables=["context", "question"]
    )

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=chroma_index.as_retriever(search_kwargs={"k": TOP_K}),
        return_source_documents=True,  # garde les sources si besoin debug
        combine_docs_chain_kwargs={"prompt": QA_PROMPT}
    )

# ===============================
# G√©n√©ration de r√©ponse
# ===============================
def generate_rag_response(rag_chain, prompt, chat_history):
    try:
        result = rag_chain.invoke({
            "question": prompt,
            "chat_history": chat_history
        })
        return result["answer"], result.get("source_documents", [])
    except Exception as e:
        return f"Erreur lors du traitement: {str(e)}", []
